{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP Tasks with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'text7' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1c537de86df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text7' is not defined"
     ]
    }
   ],
   "source": [
    "text7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(text7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 10 words in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['disclose',\n 'NYSE',\n '1986',\n 'logo',\n 'textile',\n 'shipyards',\n 'poverty',\n 'N.V',\n 'She',\n 'Monopolies']"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "list(set(text7))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency of words\n",
    "\n",
    "## Let's create distribution and get total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "12408"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "dist = FreqDist(text7)\n",
    "len(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st 10 words in lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Pierre', 'Vinken', ',', '61', 'years', 'old', 'will', 'join', 'the', 'board']"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "vocab1 = dist.keys()\n",
    "#vocab1[:10] \n",
    "# In Python 3 dict.keys() returns an iterable view instead of a list\n",
    "list(vocab1)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## freequency of word 'trading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "162"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "dist['trading']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many times a word occur\n",
    "In below, find words whose length is greater than 5\n",
    "and occur more than 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['billion',\n 'company',\n 'president',\n 'because',\n 'market',\n 'million',\n 'shares',\n 'trading',\n 'program']"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "freqwords = [w for w in vocab1 if len(w) > 5 and dist[w] > 100]\n",
    "freqwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and stemming\n",
    "stemming and Lemmatization\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word. Stemming follows an algorithm with steps to perform on the words which makes it faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem **even if the stem itself is not a valid word** in the Language.\"\n",
    "\n",
    "Stem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as (-ed,-ize, -s,-de,mis). So stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word.\n",
    "\n",
    "Example of two different stemmers\n",
    "\n",
    "|Word | Porter Stemmer | Lancaster Stemmer|\n",
    "|------|------|------|\n",
    "|friend | friend |friend|\n",
    "|friendship | friendship | friend|\n",
    "|friends |friend| friend|\n",
    "|friendships |friendship| friend|\n",
    "|stabil |stabil |stabl|\n",
    "|destabilize| destabil| dest|\n",
    "|misunderstanding |misunderstand| misunderstand|\n",
    "|railroad |railroad |railroad|\n",
    "|moonlight |moonlight| moonlight|\n",
    "|football |footbal |footbal|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['list', 'listed', 'lists', 'listing', 'listings']"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "input1 = \"List listed lists listing listings\"\n",
    "words1 = input1.lower().split(' ')\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['list', 'list', 'list', 'list', 'list']"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in words1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language**. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "For example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Universal',\n 'Declaration',\n 'of',\n 'Human',\n 'Rights',\n 'Preamble',\n 'Whereas',\n 'recognition',\n 'of',\n 'the',\n 'inherent',\n 'dignity',\n 'and',\n 'of',\n 'the',\n 'equal',\n 'and',\n 'inalienable',\n 'rights',\n 'of']"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "udhr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['univers',\n 'declar',\n 'of',\n 'human',\n 'right',\n 'preambl',\n 'wherea',\n 'recognit',\n 'of',\n 'the',\n 'inher',\n 'digniti',\n 'and',\n 'of',\n 'the',\n 'equal',\n 'and',\n 'inalien',\n 'right',\n 'of']"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "[porter.stem(t) for t in udhr[:20]] # Still Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Universal',\n 'Declaration',\n 'of',\n 'Human',\n 'Rights',\n 'Preamble',\n 'Whereas',\n 'recognition',\n 'of',\n 'the',\n 'inherent',\n 'dignity',\n 'and',\n 'of',\n 'the',\n 'equal',\n 'and',\n 'inalienable',\n 'right',\n 'of']"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "[WNlemma.lemmatize(t) for t in udhr[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  my own words\n",
    "are, slept, was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['are', 'slept']"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "[WNlemma.lemmatize(t) for t in ['are', 'slept']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, you must be wondering that no actual root form has been given for any word, this is because they are given without context. You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for pos parameter in wordnet_lemmatizer.lemmatize.\n",
    "\n",
    "**Note: Lemmatization and stemming are related, but different. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meaning depending on part of speech.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['be', 'sleep']"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Consideriing words as verb\n",
    "[WNlemma.lemmatize(t, pos=\"v\") for t in ['are', 'slept']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['runner', 'better', 'best', 'faster']\n['runners', 'well', 'best', 'faster']\n['runners', 'good', 'best', 'fast']\n"
    }
   ],
   "source": [
    "# Consideriing words as different parts of speech - n for noun, r for adverb, a for adjective\n",
    "print([WNlemma.lemmatize(t, pos=\"n\") for t in ['runners','better', 'best', 'faster']])\n",
    "print([WNlemma.lemmatize(t, pos=\"r\") for t in ['runners','better', 'best', 'faster']])\n",
    "print([WNlemma.lemmatize(t, pos=\"a\") for t in ['runners','better', 'best', 'faster']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming or Lemmatization - What to use? </br>\n",
    "Question you may be asking yourself when should I use Stemming and when should I use Lemmatization? The answer itself is in whatever you have learned from this tutorial. You have seen the following points:\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma.\n",
    "\n",
    "So when to use what! The above points show that if speed is focused then stemming should be used since lemmatizers scan a corpus which consumed time and processing. It depends on the application you are working on that decides if stemmers should be used or lemmatizers. If you are building a language application in which language is important you should use lemmatization as it uses a corpus to match root forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "text11.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see split is not doing good job i.e. '.' is part of bed itself and 'n't' is still part of shouldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Children',\n 'should',\n \"n't\",\n 'drink',\n 'a',\n 'sugary',\n 'drink',\n 'before',\n 'bed',\n '.']"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "nltk.word_tokenize(text11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting sentences\n",
    "\n",
    "Let's understand what is sentence and what are sentence bounderies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "sentences = nltk.sent_tokenize(text12)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This is the first sentence.',\n 'A gallon of milk in the U.S. costs $2.99.',\n 'Is this the third sentence?',\n 'Yes, it is!']"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NLP Tasks with NLTK\n",
    "Covers\n",
    "* POS Tagging\n",
    "* Sentence Structure\n",
    "* Semantic role labelling\n",
    "* NER\n",
    "* Co-reference Resolution and pronoun Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "MD: modal auxiliary\n    can cannot could couldn't dare may might must need ought shall should\n    shouldn't will would\n"
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('MD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('Children', 'NNP'),\n ('should', 'MD'),\n (\"n't\", 'RB'),\n ('drink', 'VB'),\n ('a', 'DT'),\n ('sugary', 'JJ'),\n ('drink', 'NN'),\n ('before', 'IN'),\n ('bed', 'NN'),\n ('.', '.')]"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "text13 = nltk.word_tokenize(text11)\n",
    "nltk.pos_tag(text13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNP: noun, proper, singular\n",
    "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
    "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
    "    Shannon A.K.C. Meltex Liverpool ...\n",
    "\n",
    "NN: noun, common, singular or mass\n",
    "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
    "    investment slide humour falloff slick wind hyena override subhumanity\n",
    "    machinist ...\n",
    "\n",
    "NNS: noun, common, plural\n",
    "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
    "    divestitures storehouses designs clubs fragrances averages\n",
    "    subjectivists apprehensions muses factory-jobs ...\n",
    "\n",
    "VB: verb, base form\n",
    "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
    "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
    "    boost brace break bring broil brush build ...\n",
    "\n",
    "VBG: verb, present participle or gerund\n",
    "    telegraphing stirring focusing angering judging stalling lactating\n",
    "    hankerin' alleging veering capping approaching traveling besieging\n",
    "    encrypting interrupting erasing wincing ...\n",
    "    \n",
    "MD: modal auxiliary\n",
    "    can cannot could couldn't dare may might must need ought shall should\n",
    "    shouldn't will would\n",
    "\n",
    "RB: adverb\n",
    "    occasionally unabatingly maddeningly adventurously professedly\n",
    "    stirringly prominently technologically magisterially predominately\n",
    "    swiftly fiscally pitilessly ...\n",
    "\n",
    "DT: determiner\n",
    "    all an another any both del each either every half la many much nary\n",
    "    neither no some such that the them these this those\n",
    "\n",
    "JJ: adjective or numeral, ordinal\n",
    "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
    "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
    "    multilingual multi-disciplinary ...\n",
    "\n",
    "IN: preposition or conjunction, subordinating\n",
    "    astride among uppon whether out inside pro despite on by throughout\n",
    "    below within for towards near behind atop around if like until below\n",
    "    next into if beside ...\n",
    "\n",
    ".: sentence terminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "VB: verb, base form\n    ask assemble assess assign assume atone attention avoid bake balkanize\n    bank begin behold believe bend benefit bevel beware bless boil bomb\n    boost brace break bring broil brush build ...\nNone\n"
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset('VB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguity in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('Visiting', 'VBG'),\n ('aunts', 'NNS'),\n ('can', 'MD'),\n ('be', 'VB'),\n ('a', 'DT'),\n ('nuisance', 'NN')]"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "text14 = nltk.word_tokenize(\"Visiting aunts can be a nuisance\")\n",
    "nltk.pos_tag(text14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, above 'Visting' can be JJ (Adjuective as well) i.e. Aunt who is visiting as well.\n",
    "\n",
    "Note that, NLTK doesn't all possible POS rather gives most likly POS tagging. As visiting is mostly used as Gerund, VBG is given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing sentence structure\n",
    "\n",
    "### Let's crate our own grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
    }
   ],
   "source": [
    "# Parsing sentence structure\n",
    "text15 = nltk.word_tokenize(\"Alice loves Bob\")\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP\n",
    "NP -> 'Alice' | 'Bob'\n",
    "V -> 'loves'\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "trees = parser.parse_all(text15)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let take sentence with amibiguoous meaning - see below - did you see a man holding telescope or did you use telescope to see the man?\n",
    "\n",
    "and parse that with custom grammer rules we define above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<Grammar with 13 productions>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "text16 = nltk.word_tokenize(\"I saw the man with a telescope\")\n",
    "grammar1 = nltk.data.load('mygrammar.cfg')\n",
    "grammar1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and see bleow for grammer rules tree created by nltk based on custom grammer rules we defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S\n  (NP I)\n  (VP\n    (VP (V saw) (NP (Det the) (N man)))\n    (PP (P with) (NP (Det a) (N telescope)))))\n(S\n  (NP I)\n  (VP\n    (V saw)\n    (NP (Det the) (N man) (PP (P with) (NP (Det a) (N telescope))))))\n"
    }
   ],
   "source": [
    "parser = nltk.ChartParser(grammar1)\n",
    "trees = parser.parse_all(text16)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## treebank: it is grammer rule created by many people together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S\n  (NP-SBJ\n    (NP (NNP Pierre) (NNP Vinken))\n    (, ,)\n    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n    (, ,))\n  (VP\n    (MD will)\n    (VP\n      (VB join)\n      (NP (DT the) (NN board))\n      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n      (NP-TMP (NNP Nov.) (CD 29))))\n  (. .))\n"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "text17 = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "print(text17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging and parsing ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('The', 'DT'), ('old', 'JJ'), ('man', 'NN'), ('the', 'DT'), ('boat', 'NN')]"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "text18 = nltk.word_tokenize(\"The old man the boat\")\n",
    "nltk.pos_tag(text18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('Colorless', 'NNP'),\n ('green', 'JJ'),\n ('ideas', 'NNS'),\n ('sleep', 'VBP'),\n ('furiously', 'RB')]"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "text19 = nltk.word_tokenize(\"Colorless green ideas sleep furiously\")\n",
    "nltk.pos_tag(text19)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python36964bitvenvvenv6377c2a86e5e439e8ad388bbc64dbe14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}